{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C-kf8Xg7Y3T9"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "import torch.optim as optim, torch.utils.data as data\n",
        "import torchvision.datasets as datasets\n",
        "import time, copy, random, os, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.autoaugment import AutoAugmentPolicy\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import random_split\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "INPUT_SIZE = (3, 32, 32)\n",
        "EPOCHS = 150\n",
        "LEARNING_RATE = 1e-3\n"
      ],
      "metadata": {
        "id": "0zxGQmq6eIHW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKIsJbu7dVwT",
        "outputId": "a39b4edc-4368-4c07-d62a-e4576c9928fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 10 23:42:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "aS6W6SIcdk5G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets the seed for numpy and torch to make sure functions with a random component behave deterministically\n",
        "# torch.backends.cudnn.deterministic = true sets the CuDNN to deterministic mode.\n",
        "# This function allows us to run experiments 100% deterministically.\n",
        "# https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "0N5DwCYsdKo1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  Load and Batch the Data"
      ],
      "metadata": {
        "id": "2pORWSiUdu7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(batch_size, root = '.data', valid_ratio = 0.1):\n",
        "  \n",
        "    #means and stds for the 3 channels of the dataset\n",
        "    means = (0.4914, 0.4822, 0.4465)\n",
        "    stds = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "    #   Performs the following data augmentations on the training set:\n",
        "    #       1.  Random Rotation:  \n",
        "    #       2.  RandomHorizontalFlip\n",
        "    #       3.  Random Crop\n",
        "    #       4.  AutoAugment: AutoAugmention policies learned from the CIFAR10 dataset\n",
        "    #           AutoAugment: Learning Augmentation Strategies from Data\" <https://arxiv.org/pdf/1805.09501.pdf>`\n",
        "    train_transforms = transforms.Compose([\n",
        "                           #transforms.RandomRotation(5),\n",
        "                           #transforms.RandomHorizontalFlip(0.5),\n",
        "                           #transforms.RandomCrop(32, padding = 2),\n",
        "                           #transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
        "                           transforms.ToTensor(),  # convert data to torch.FloatTensor: CxHxW\n",
        "                           transforms.Normalize(mean = means, std = stds, inplace=True), #Range is in [-1,1], data = (data - mean)/std\n",
        "                       ])\n",
        "\n",
        "    test_transforms = transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean = means, std = stds, inplace=True),\n",
        "                       ])\n",
        "\n",
        "    # choose the training and test datasets\n",
        "    train_data = datasets.CIFAR10(root, \n",
        "                              train = True, \n",
        "                              download = True, \n",
        "                              transform = train_transforms)\n",
        "    \n",
        "    test_data = datasets.CIFAR10(root, \n",
        "                             train = False, \n",
        "                             download = True, \n",
        "                             transform = test_transforms)\n",
        "    \n",
        "    # obtain number of training/validation examples that will be used to create the respective datasets \n",
        "    n_train_examples = int((1 - valid_ratio) * len(train_data))\n",
        "    n_valid_examples = int(valid_ratio* len(train_data))\n",
        "\n",
        "    train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
        "    \n",
        "    #Creates a deep copy\n",
        "    valid_data = copy.deepcopy(valid_data)  \n",
        "    valid_data.dataset.transform = test_transforms\n",
        "\n",
        "    # load training data in batches\n",
        "    train_dataloader = data.DataLoader(train_data,\n",
        "                                 shuffle = True,\n",
        "                                 batch_size = batch_size,\n",
        "                                 num_workers=2, \n",
        "                                 pin_memory=True)   #pin_memory speeds transfer of data from CPU to GPU\n",
        "\n",
        "    # load validation data in batches\n",
        "    valid_dataloader = data.DataLoader(valid_data,\n",
        "                                 batch_size = batch_size,\n",
        "                                 num_workers=2, \n",
        "                                 pin_memory=True)\n",
        "\n",
        "    # load test data in batches\n",
        "    test_dataloader = data.DataLoader(test_data,\n",
        "                                batch_size = batch_size,\n",
        "                                num_workers=2, \n",
        "                                pin_memory=True)\n",
        "  \n",
        "    return train_dataloader, valid_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "IhH-Oy98ZYMR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and batch the data\n",
        "train_dataloader, valid_dataloader, test_dataloader = get_data_loaders(BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4DF_d-weBAm",
        "outputId": "7e63ade0-fbf5-4c3e-a8ef-321ed8c24166"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to .data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13124876.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/cifar-10-python.tar.gz to .data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Define the Network"
      ],
      "metadata": {
        "id": "0jsE8g_LZoOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "\n",
        "#Residual Block\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=3,padding=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "#1. Convolutional Layer: conv1\n",
        "#2. Residual Layers: layer1, layer2, layer3, layer4\n",
        "#3. Fully Connected Layer: linear\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 32\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "#ResNet18 has 4 residual layers, each composed of a specifed number of residual blocks\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [5,4,3,2])\n"
      ],
      "metadata": {
        "id": "EIxB9njiZq5U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  Specify Model, Loss Function, Optimizer, and Scheduler"
      ],
      "metadata": {
        "id": "mguuWb2MefkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model, loss function, and optimizer; move model to device\n",
        "model = ResNet18().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# Specify scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n"
      ],
      "metadata": {
        "id": "GIB_kgmNejIb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check total number of parameters\n",
        "summary(model, input_size=INPUT_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFzNX4yZfJ6-",
        "outputId": "97183f9b-3a2e-49b7-f78f-97eb377cbde4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "            Conv2d-3           [-1, 64, 32, 32]          18,432\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "            Conv2d-7           [-1, 64, 32, 32]          18,432\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-9           [-1, 64, 32, 32]               0\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "           Conv2d-12           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-14           [-1, 64, 32, 32]               0\n",
            "           Conv2d-15           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
            "           Conv2d-17           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-19           [-1, 64, 32, 32]               0\n",
            "           Conv2d-20           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 32, 32]             128\n",
            "           Conv2d-22           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-24           [-1, 64, 32, 32]               0\n",
            "           Conv2d-25           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
            "           Conv2d-27           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-28           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-29           [-1, 64, 32, 32]               0\n",
            "           Conv2d-30          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
            "           Conv2d-32          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-33          [-1, 128, 16, 16]             256\n",
            "           Conv2d-34          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-36          [-1, 128, 16, 16]               0\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "           Conv2d-39          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-40          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-41          [-1, 128, 16, 16]               0\n",
            "           Conv2d-42          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
            "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-46          [-1, 128, 16, 16]               0\n",
            "           Conv2d-47          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-48          [-1, 128, 16, 16]             256\n",
            "           Conv2d-49          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-51          [-1, 128, 16, 16]               0\n",
            "           Conv2d-52            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 8, 8]             256\n",
            "           Conv2d-54            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-55            [-1, 128, 8, 8]             256\n",
            "           Conv2d-56            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-57            [-1, 128, 8, 8]             256\n",
            "       BasicBlock-58            [-1, 128, 8, 8]               0\n",
            "           Conv2d-59            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-60            [-1, 128, 8, 8]             256\n",
            "           Conv2d-61            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-62            [-1, 128, 8, 8]             256\n",
            "       BasicBlock-63            [-1, 128, 8, 8]               0\n",
            "           Conv2d-64            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-65            [-1, 128, 8, 8]             256\n",
            "           Conv2d-66            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-67            [-1, 128, 8, 8]             256\n",
            "       BasicBlock-68            [-1, 128, 8, 8]               0\n",
            "           Conv2d-69            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-70            [-1, 256, 4, 4]             512\n",
            "           Conv2d-71            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-72            [-1, 256, 4, 4]             512\n",
            "           Conv2d-73            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-74            [-1, 256, 4, 4]             512\n",
            "       BasicBlock-75            [-1, 256, 4, 4]               0\n",
            "           Conv2d-76            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
            "           Conv2d-78            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-79            [-1, 256, 4, 4]             512\n",
            "       BasicBlock-80            [-1, 256, 4, 4]               0\n",
            "           Linear-81                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,951,338\n",
            "Trainable params: 4,951,338\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 20.94\n",
            "Params size (MB): 18.89\n",
            "Estimated Total Size (MB): 39.84\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Training and Validation"
      ],
      "metadata": {
        "id": "XiMFn7HYffbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy is the fraction of predictions the model got correct\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "# time interval for each epoch\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "QX3sZ5hoZEOy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    # Tracks training loss and accuracy as model trains\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    #lrs = []\n",
        "\n",
        "    # Prep model for training\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in iterator:\n",
        "        # Move inputs and labels to device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        # Clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model        \n",
        "        y_pred = model(x)\n",
        "        \n",
        "        # Calculate the loss\n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        # Calculate the accuracy\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "        \n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "\n",
        "        #OneCycleLr Scheduler\n",
        "        #scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "8G3N_MCHkz05"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, device):\n",
        "    # Keeps track of validation loss and  accuracy\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # Prep model for evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward Pass\n",
        "            y_pred = model(x)\n",
        "            # Calculate the validation loss\n",
        "            loss = criterion(y_pred, y)\n",
        "            #Calculate the validation accuracy\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "            #Record validation loss\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "HlQfhhAaf5Lo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Validate the model:\n",
        "\n",
        "# Tracks average training loss and accuracy per epoch\n",
        "#  and tracks average validation loss and accuracy per epoch\n",
        "Train_Loss = []\n",
        "Train_Acc = []\n",
        "Val_Loss = []\n",
        "Val_Acc = []\n",
        "\n",
        "\n",
        "# Tracks best validation loss\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "    # Epoch start time\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion, device)\n",
        "\n",
        "    # OnReducePlateau is a scheduler monitoring validation loss\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "    # Saves model with best validation loss in a state dictionary\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(),'Model_4.pt')\n",
        "    # Epoch end time\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    #MultistepLR\n",
        "    #scheduler.step()\n",
        "\n",
        "    # Prints training/validation statistics as well as epoch time\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} || EpochTime: {epoch_mins}m{epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} || Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val Loss: {valid_loss:.3f} ||  Val Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    Train_Loss.append(train_loss)\n",
        "    Train_Acc.append(train_acc)\n",
        "    Val_Loss.append(valid_loss)\n",
        "    Val_Acc.append(valid_acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWLqb6XVgAhz",
        "outputId": "e3f3db8c-7813-428b-8cbd-ebad1df94a0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 || EpochTime: 0m14s\n",
            "\tTrain Loss: 1.370 || Train Acc: 49.92%\n",
            "\t Val Loss: 1.114 ||  Val Acc: 59.38%\n",
            "Epoch: 02 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.864 || Train Acc: 69.45%\n",
            "\t Val Loss: 0.769 ||  Val Acc: 73.00%\n",
            "Epoch: 03 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.641 || Train Acc: 77.38%\n",
            "\t Val Loss: 0.688 ||  Val Acc: 76.09%\n",
            "Epoch: 04 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.511 || Train Acc: 82.34%\n",
            "\t Val Loss: 0.570 ||  Val Acc: 80.78%\n",
            "Epoch: 05 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.409 || Train Acc: 85.76%\n",
            "\t Val Loss: 0.571 ||  Val Acc: 80.93%\n",
            "Epoch: 06 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.321 || Train Acc: 88.74%\n",
            "\t Val Loss: 0.562 ||  Val Acc: 82.00%\n",
            "Epoch: 07 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.250 || Train Acc: 91.14%\n",
            "\t Val Loss: 0.617 ||  Val Acc: 81.33%\n",
            "Epoch: 08 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.193 || Train Acc: 93.32%\n",
            "\t Val Loss: 0.539 ||  Val Acc: 84.63%\n",
            "Epoch: 09 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.145 || Train Acc: 94.93%\n",
            "\t Val Loss: 0.603 ||  Val Acc: 83.31%\n",
            "Epoch: 10 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.112 || Train Acc: 96.03%\n",
            "\t Val Loss: 0.628 ||  Val Acc: 83.17%\n",
            "Epoch: 11 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.092 || Train Acc: 96.74%\n",
            "\t Val Loss: 0.630 ||  Val Acc: 83.78%\n",
            "Epoch: 12 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.074 || Train Acc: 97.49%\n",
            "\t Val Loss: 0.678 ||  Val Acc: 83.70%\n",
            "Epoch: 13 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.069 || Train Acc: 97.61%\n",
            "\t Val Loss: 0.700 ||  Val Acc: 82.83%\n",
            "Epoch: 14 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.053 || Train Acc: 98.23%\n",
            "\t Val Loss: 0.683 ||  Val Acc: 84.30%\n",
            "Epoch: 15 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.054 || Train Acc: 98.13%\n",
            "\t Val Loss: 0.724 ||  Val Acc: 84.10%\n",
            "Epoch: 16 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.046 || Train Acc: 98.43%\n",
            "\t Val Loss: 0.792 ||  Val Acc: 83.35%\n",
            "Epoch: 17 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.046 || Train Acc: 98.38%\n",
            "\t Val Loss: 0.720 ||  Val Acc: 84.77%\n",
            "Epoch: 18 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.043 || Train Acc: 98.51%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 84.08%\n",
            "Epoch: 19 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.037 || Train Acc: 98.72%\n",
            "\t Val Loss: 0.803 ||  Val Acc: 83.72%\n",
            "Epoch: 20 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.012 || Train Acc: 99.65%\n",
            "\t Val Loss: 0.658 ||  Val Acc: 86.06%\n",
            "Epoch: 21 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.003 || Train Acc: 99.94%\n",
            "\t Val Loss: 0.671 ||  Val Acc: 86.39%\n",
            "Epoch: 22 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.002 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.680 ||  Val Acc: 86.16%\n",
            "Epoch: 23 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.002 || Train Acc: 99.97%\n",
            "\t Val Loss: 0.677 ||  Val Acc: 86.47%\n",
            "Epoch: 24 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.697 ||  Val Acc: 86.55%\n",
            "Epoch: 25 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.99%\n",
            "\t Val Loss: 0.704 ||  Val Acc: 86.79%\n",
            "Epoch: 26 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.001 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.707 ||  Val Acc: 86.97%\n",
            "Epoch: 27 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.721 ||  Val Acc: 86.89%\n",
            "Epoch: 28 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.743 ||  Val Acc: 86.45%\n",
            "Epoch: 29 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.783 ||  Val Acc: 86.93%\n",
            "Epoch: 30 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.99%\n",
            "\t Val Loss: 0.781 ||  Val Acc: 86.57%\n",
            "Epoch: 31 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.762 ||  Val Acc: 86.51%\n",
            "Epoch: 32 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.91%\n",
            "Epoch: 33 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 87.01%\n",
            "Epoch: 34 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.782 ||  Val Acc: 86.99%\n",
            "Epoch: 35 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.769 ||  Val Acc: 86.83%\n",
            "Epoch: 36 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.767 ||  Val Acc: 86.95%\n",
            "Epoch: 37 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.61%\n",
            "Epoch: 38 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.771 ||  Val Acc: 86.97%\n",
            "Epoch: 39 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.779 ||  Val Acc: 86.83%\n",
            "Epoch: 40 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.89%\n",
            "Epoch: 41 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.782 ||  Val Acc: 86.83%\n",
            "Epoch: 42 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.776 ||  Val Acc: 86.71%\n",
            "Epoch: 43 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.75%\n",
            "Epoch: 44 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.768 ||  Val Acc: 86.83%\n",
            "Epoch: 45 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.89%\n",
            "Epoch: 46 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.96%\n",
            "\t Val Loss: 0.765 ||  Val Acc: 87.06%\n",
            "Epoch: 47 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.768 ||  Val Acc: 86.97%\n",
            "Epoch: 48 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.781 ||  Val Acc: 86.67%\n",
            "Epoch: 49 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.85%\n",
            "Epoch: 50 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.91%\n",
            "Epoch: 51 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.73%\n",
            "Epoch: 52 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.780 ||  Val Acc: 86.71%\n",
            "Epoch: 53 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.93%\n",
            "Epoch: 54 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.777 ||  Val Acc: 86.93%\n",
            "Epoch: 55 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.776 ||  Val Acc: 87.06%\n",
            "Epoch: 56 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.782 ||  Val Acc: 86.91%\n",
            "Epoch: 57 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.73%\n",
            "Epoch: 58 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 87.05%\n",
            "Epoch: 59 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.81%\n",
            "Epoch: 60 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.777 ||  Val Acc: 87.08%\n",
            "Epoch: 61 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.766 ||  Val Acc: 86.79%\n",
            "Epoch: 62 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 87.20%\n",
            "Epoch: 63 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.782 ||  Val Acc: 86.91%\n",
            "Epoch: 64 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 87.08%\n",
            "Epoch: 65 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.85%\n",
            "Epoch: 66 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.67%\n",
            "Epoch: 67 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 86.81%\n",
            "Epoch: 68 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.779 ||  Val Acc: 86.59%\n",
            "Epoch: 69 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.767 ||  Val Acc: 86.97%\n",
            "Epoch: 70 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.776 ||  Val Acc: 86.73%\n",
            "Epoch: 71 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.784 ||  Val Acc: 86.97%\n",
            "Epoch: 72 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 86.89%\n",
            "Epoch: 73 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.99%\n",
            "Epoch: 74 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.763 ||  Val Acc: 87.18%\n",
            "Epoch: 75 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.771 ||  Val Acc: 87.03%\n",
            "Epoch: 76 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.91%\n",
            "Epoch: 77 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.775 ||  Val Acc: 87.01%\n",
            "Epoch: 78 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.93%\n",
            "Epoch: 79 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.767 ||  Val Acc: 86.81%\n",
            "Epoch: 80 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.775 ||  Val Acc: 87.08%\n",
            "Epoch: 81 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 86.97%\n",
            "Epoch: 82 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.75%\n",
            "Epoch: 83 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.765 ||  Val Acc: 86.93%\n",
            "Epoch: 84 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.775 ||  Val Acc: 86.93%\n",
            "Epoch: 85 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.771 ||  Val Acc: 86.97%\n",
            "Epoch: 86 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.769 ||  Val Acc: 86.87%\n",
            "Epoch: 87 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.81%\n",
            "Epoch: 88 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 87.18%\n",
            "Epoch: 89 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.783 ||  Val Acc: 86.69%\n",
            "Epoch: 90 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.779 ||  Val Acc: 87.01%\n",
            "Epoch: 91 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.83%\n",
            "Epoch: 92 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.769 ||  Val Acc: 87.06%\n",
            "Epoch: 93 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.99%\n",
            "Epoch: 94 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 87.12%\n",
            "Epoch: 95 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.786 ||  Val Acc: 86.51%\n",
            "Epoch: 96 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.784 ||  Val Acc: 86.87%\n",
            "Epoch: 97 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.777 ||  Val Acc: 87.08%\n",
            "Epoch: 98 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.765 ||  Val Acc: 87.12%\n",
            "Epoch: 99 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.771 ||  Val Acc: 87.05%\n",
            "Epoch: 100 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.785 ||  Val Acc: 87.12%\n",
            "Epoch: 101 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.73%\n",
            "Epoch: 102 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.769 ||  Val Acc: 86.95%\n",
            "Epoch: 103 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.775 ||  Val Acc: 87.03%\n",
            "Epoch: 104 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.784 ||  Val Acc: 86.79%\n",
            "Epoch: 105 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.775 ||  Val Acc: 86.91%\n",
            "Epoch: 106 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.93%\n",
            "Epoch: 107 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.73%\n",
            "Epoch: 108 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.782 ||  Val Acc: 86.83%\n",
            "Epoch: 109 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.87%\n",
            "Epoch: 110 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.767 ||  Val Acc: 87.12%\n",
            "Epoch: 111 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.765 ||  Val Acc: 87.05%\n",
            "Epoch: 112 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.782 ||  Val Acc: 86.91%\n",
            "Epoch: 113 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.780 ||  Val Acc: 86.73%\n",
            "Epoch: 114 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.781 ||  Val Acc: 86.99%\n",
            "Epoch: 115 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.767 ||  Val Acc: 86.87%\n",
            "Epoch: 116 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.760 ||  Val Acc: 87.08%\n",
            "Epoch: 117 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.783 ||  Val Acc: 86.63%\n",
            "Epoch: 118 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.55%\n",
            "Epoch: 119 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.781 ||  Val Acc: 86.85%\n",
            "Epoch: 120 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.93%\n",
            "Epoch: 121 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.780 ||  Val Acc: 87.01%\n",
            "Epoch: 122 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 86.89%\n",
            "Epoch: 123 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.95%\n",
            "Epoch: 124 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.81%\n",
            "Epoch: 125 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.771 ||  Val Acc: 86.73%\n",
            "Epoch: 126 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.776 ||  Val Acc: 86.79%\n",
            "Epoch: 127 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.779 ||  Val Acc: 86.55%\n",
            "Epoch: 128 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.779 ||  Val Acc: 86.91%\n",
            "Epoch: 129 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.777 ||  Val Acc: 86.97%\n",
            "Epoch: 130 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.95%\n",
            "Epoch: 131 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.781 ||  Val Acc: 86.87%\n",
            "Epoch: 132 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.780 ||  Val Acc: 86.89%\n",
            "Epoch: 133 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.81%\n",
            "Epoch: 134 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.772 ||  Val Acc: 86.73%\n",
            "Epoch: 135 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.769 ||  Val Acc: 86.73%\n",
            "Epoch: 136 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 86.85%\n",
            "Epoch: 137 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.764 ||  Val Acc: 86.91%\n",
            "Epoch: 138 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.776 ||  Val Acc: 86.89%\n",
            "Epoch: 139 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.780 ||  Val Acc: 87.01%\n",
            "Epoch: 140 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.777 ||  Val Acc: 87.12%\n",
            "Epoch: 141 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.778 ||  Val Acc: 86.59%\n",
            "Epoch: 142 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.775 ||  Val Acc: 86.65%\n",
            "Epoch: 143 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.773 ||  Val Acc: 86.85%\n",
            "Epoch: 144 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.770 ||  Val Acc: 86.85%\n",
            "Epoch: 145 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.761 ||  Val Acc: 86.93%\n",
            "Epoch: 146 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.001 || Train Acc: 99.98%\n",
            "\t Val Loss: 0.780 ||  Val Acc: 87.30%\n",
            "Epoch: 147 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.785 ||  Val Acc: 87.06%\n",
            "Epoch: 148 || EpochTime: 0m13s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.774 ||  Val Acc: 86.85%\n",
            "Epoch: 149 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.764 ||  Val Acc: 86.91%\n",
            "Epoch: 150 || EpochTime: 0m14s\n",
            "\tTrain Loss: 0.000 || Train Acc: 100.00%\n",
            "\t Val Loss: 0.787 ||  Val Acc: 86.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the loss as the network trained\n",
        "def get_loss_plot(train_loss, val_loss):\n",
        "    plt.plot(range(len(train_loss)),train_loss)\n",
        "    plt.plot(range(len(train_loss)),val_loss)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title('Loss')\n",
        "    plt.legend(['Training Loss', 'Validation Loss'])\n",
        "    plt.savefig('Model_4.png')\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "# Prints training loss and validation loss \n",
        "get_loss_plot(Train_Loss, Val_Loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "aDTjlQ2mhTD0",
        "outputId": "6403e50b-d616-4864-98f6-1270f9b75eeb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.  Evaluate Best Saved Model on Test Dataset"
      ],
      "metadata": {
        "id": "U1sV811ZiemP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model's parameters\n",
        "model.load_state_dict(torch.load('Model_4.pt'))\n",
        "\n",
        "# Calculate test loss and accuracy\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} || Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5qiQY3MiPdy",
        "outputId": "da015014-bfc9-4b09-8035-967f820655cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.535 || Test Acc: 84.02%\n"
          ]
        }
      ]
    }
  ]
}